from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader, PyPDFLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)

class ChatCallbackHandler(BaseCallbackHandler):

    def __init__(self, role):
        self.role = role
        self.message = ""
        self.message_box = None

    def on_llm_start(self, *args, **kwargs):
        self.message = ""
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, self.role)
        if self.message_box:
            self.message_box.empty()
            self.message_box = None

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        if self.message_box:
            self.message_box.markdown(self.message)

llm_gm = ChatOpenAI(
    temperature=0.7,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(role="ai"),
    ],
)

llm_player = ChatOpenAI(
    temperature=0.7,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(role="human"),
    ],
)

@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
    )
    loader = PyPDFLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})
    if 'message_box' in st.session_state:
        st.session_state['message_box'].empty()
        del st.session_state['message_box']

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )

def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

prompt_gm = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            ë‹¹ì‹ ì€ ì´ í•œêµ­ì–´ ë¡¤í”Œë ˆì‰ ê²Œì„ì˜ ê²Œì„ë§ˆìŠ¤í„°ì…ë‹ˆë‹¤.
            ì´ ê²Œì„ì€ ì¼ë°˜ì ìœ¼ë¡œ D&Dì˜ ë£°ì„ ë”°ë¦…ë‹ˆë‹¤.
            ì´ ê²Œì„ì€ ë†’ì€ ë‚œì´ë„ë¡œ ìˆ¨ê²¨ì§„ í•¨ì • í˜¹ì€ ìƒë¬¼ë“¤ì€ ëŒ€ê°œ í”Œë ˆì´ì–´ì—ê²Œ ìœ„í˜‘ì ì…ë‹ˆë‹¤.
            the Prison of Ganoì˜ ì •ë³´ì™€ ì´ì „ ë©”ì‹œì§€ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì´ ì°¨ë¡€ì— ëŒ€í•œ ì„¤ëª…ì„ ì œì‹œí•©ë‹ˆë‹¤.
            í”Œë ˆì´ì–´ì˜ ë§ˆì§€ë§‰ ì•¡ì…˜ì„ ë°”íƒ•ìœ¼ë¡œ ì´ ê²Œì„ì˜ ë‹¤ìŒ ì¥ë©´ì„ ë§Œë“­ë‹ˆë‹¤.
            ë‹¹ì‹ ì€ ê²Œì„ ë§ˆìŠ¤í„°ë¡œì¨, í”Œë ˆì´ì–´ê°€ ì²˜í•œ ë‹¤ìŒ ìƒí™©ë§Œì„ ë¬˜ì‚¬í•´ì•¼í•©ë‹ˆë‹¤.
            ë¬˜ì‚¬ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” ì „íˆ¬ ìƒí™©, í˜„ì¬ ë°©ì— ì¡´ì¬í•˜ëŠ” ì§€í˜•ë¬¼(ë¬¸, ê¸°ë‘¥, ì¥ì• ë¬¼) ë“±ì´ ìˆìŠµë‹ˆë‹¤.
            ì ˆëŒ€ í”Œë ˆì´ì–´ì˜ í–‰ë™ì„ ìœ ì¶”í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ì§„í–‰í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
            ë¬˜ì‚¬ëŠ” ì†Œì„¤ê³¼ ê°™ì´ ë¬˜ì‚¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
            
            ì •ë³´: {context}
            ì´ì „ ë©”ì‹œì§€ë“¤: {history}
            """,
        ),
        ("human", "{action}"),
    ]
)

prompt_player = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            ë‹¹ì‹ ì€ ì´ í•œêµ­ì–´ ë¡¤í”Œë ˆì‰ ê²Œì„ì˜ í”Œë ˆì´ì–´ë¡œ, êµ‰ì¥íˆ ëª¨í—˜ì ì´ê³  ì „íˆ¬ì— ëŠ¥í•˜ì§€ë§Œ í•¨ì •ì„ ì°¾ê±°ë‚˜ í•´ì œì—ëŠ” ì„œíˆ¬ë¦…ë‹ˆë‹¤.
            ê²Œì„ ë§ˆìŠ¤í„°ì™€ ë‹¹ì‹ ê³¼ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ì„ í†µí•´ ì•¡ì…˜ì— ëŒ€í•´ ê²°ì •í•´ì•¼í•©ë‹ˆë‹¤.
            ë‹¹ì‹ ì€ ë‹¹ì‹ ì— í–‰ë™ì— ëŒ€í•œ ì¼ê´€ì ì¸ í–‰ë™ ë°©ì‹ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
            ë‹¹ì‹ ì€ í”Œë ˆì´ì–´ë¡œì¨ ì£¼ì–´ì§„ ì •ë³´ë¥¼ í† ëŒ€ë¡œë§Œ í–‰ë™í•´ì•¼ í•©ë‹ˆë‹¤.
            ì ˆëŒ€ ë‹¤ìŒ ìƒí™©ì„ ìœ ì¶”í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ì§„í–‰í•´ì„œëŠ” ì•ˆë˜ê³  ì‹¤í–‰í•  í–‰ë™ë§Œì„ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
            
            ì´ì „ ë©”ì‹œì§€ë“¤: {history}
            """,
        ),
        ("human", "{action}"),
    ]
)

# ê¸°ì¡´ ì½”ë“œì—ì„œ íŒŒì¼ ì„ íƒ ë° ì—…ë¡œë“œ ë¶€ë¶„ì„ ì œê±°í•˜ê³ , ê³ ì •ëœ íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
cache_dir = LocalFileStore("./.cache/embeddings/The Prison of Gano 03.pdf")
splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
)
loader = PyPDFLoader("./.cache/files/The Prison of Gano 03.pdf")
docs = loader.load_and_split(text_splitter=splitter)
embeddings = OpenAIEmbeddings()
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
vectorstore = FAISS.from_documents(docs, cached_embeddings)
retriever = vectorstore.as_retriever()

st.title("The Prsion of Gano")

st.markdown(
    """
ì–´ë‘  ì†ì—ì„œ, ì˜¤ë˜ëœ ê°ì˜¥ "ê°€ë…¸ì˜ ê°ì˜¥"ì´ ìˆ¨ì–´ ìˆë‹¤.
ì´ ê°ì˜¥ì€ ìˆ˜ë§ì€ ëª¨í—˜ê°€ë“¤ì´ ë„ì „í•˜ì˜€ìœ¼ë‚˜ ëŒì•„ì˜¤ì§€ ëª»í•œ ê³³ìœ¼ë¡œ ì•…ëª…ì´ ë†’ë‹¤.
ê°ì˜¥ì˜ ë²½ì€ ë›°ì–´ë‚œ ì„ì¬ë¡œ ì§€ì–´ì¡Œê³ , í”Œë˜ê·¸ìŠ¤í†¤ìœ¼ë¡œ ëœ ë°”ë‹¥ì€ ë°œì†Œë¦¬ë¥¼ ìˆ¨ê¸°ê¸° ì–´ë ¤ìš´ êµ¬ì¡°ì´ë‹¤.
ê°ì˜¥ ë‚´ë¶€ëŠ” ë”°ëœ»í•˜ì§€ë§Œ, í¬ë¯¸í•œ ë¹›ë§Œì´ ê°„ê°„ì´ ë¹„ì¶”ì–´ ì•ì„ ê°€ëŠ í•˜ê¸° ì–´ë ¤ìš´ ê³³ì´ë‹¤.
"""
)

if "messages" not in st.session_state:
    st.session_state["messages"] = []

def get_last_message(role):
    for msg in reversed(st.session_state["messages"]):
        if msg["role"] == role:
            return msg["message"]
    return ""

if st.button("Next Turn"):
    history = "\n".join(f"{msg['role']}: {msg['message']}" for msg in st.session_state["messages"])
    if len(st.session_state["messages"]) % 2 == 0:
        # ê²Œì„ ë§ˆìŠ¤í„°ì˜ ì°¨ë¡€
        last_player_action = get_last_message("human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "history": lambda x: history,
                "action": RunnablePassthrough(),
            }
            | prompt_gm
            | llm_gm
        )
        with st.chat_message("ai"):
            chain.invoke(last_player_action)
    else:
        # í”Œë ˆì´ì–´ì˜ ì°¨ë¡€
        last_gm_action = get_last_message("ai")
        chain = (
            {
                "history": lambda x: history,
                "action": RunnablePassthrough(),
            }
            | prompt_player
            | llm_player
        )
        with st.chat_message("human"):
            chain.invoke(last_gm_action)
    
    paint_history()

else:
    st.session_state["messages"] = []